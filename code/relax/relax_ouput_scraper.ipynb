{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os \n",
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i in enumerate(f,1):\n",
    "            pass\n",
    "    return i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_line(string, file):\n",
    "    counter = 0\n",
    "    with open(file) as file:\n",
    "        for num, line in enumerate(file, 0):\n",
    "            if string in line:\n",
    "                counter = num\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_line(string, file):\n",
    "    with open(file) as file:\n",
    "        for num, line in enumerate(file, 0):\n",
    "            if string in line:\n",
    "                break\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_natoms(relax_output_file):\n",
    "     \n",
    "    file = open(relax_output_file)\n",
    "    for line in file:\n",
    "        x = re.findall('number of atoms', line)\n",
    "        if len(x)>0:\n",
    "            natoms = re.sub(\"[^0-9]\", \"\", line)\n",
    "            natoms = int(natoms)\n",
    "    return natoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relax_dict(relax_output_file, pickle_name, struct_file_name):\n",
    "    \n",
    "    file = open(relax_output_file)\n",
    "    struct_file = open(struct_file_name, 'w+')\n",
    "    lines = file.readlines()\n",
    "    relax_dict = {}\n",
    "    \n",
    "    natoms = get_natoms(relax_output_file)\n",
    "    relax_dict['natoms'] = natoms\n",
    "            \n",
    "    num_lines = file_len(relax_output_file)\n",
    "    #Relaxed Structure\n",
    "    for k in range(num_lines):\n",
    "        line = lines[k]\n",
    "        if 'Begin final coordinates' in line:\n",
    "            for i in range(k+3, k+9+natoms):\n",
    "                struct_file.write(lines[i])\n",
    "    struct_file.write('\\n')\n",
    "    struct_file.close()\n",
    "    file.close()\n",
    "    \n",
    "    stress_line = last_line('total   stress', relax_output_file)\n",
    "    \n",
    "    #Row vectors making up the total stress tensor(Ry/bohr**3)\n",
    "    total1 = nltk.word_tokenize(lines[stress_line +1])[0:3]\n",
    "    total1 = np.array([float(k) for k in total1])\n",
    "    total2 = nltk.word_tokenize(lines[stress_line +2])[0:3]\n",
    "    total2 = np.array([float(k) for k in total2])\n",
    "    total3 = nltk.word_tokenize(lines[stress_line +3])[0:3]\n",
    "    total3 = np.array([float(k) for k in total3])\n",
    "    \n",
    "    total_stress = np.array([total1, total2, total3])\n",
    "    relax_dict['total_stress'] = total_stress\n",
    "    \n",
    "    #Kinetic stress tensor\n",
    "    kin1 = nltk.word_tokenize(lines[stress_line +5])[-3:]\n",
    "    kin1 = np.array([float(k) for k in kin1])\n",
    "    kin2 = nltk.word_tokenize(lines[stress_line +6])[-3:]\n",
    "    kin2 = np.array([float(k) for k in kin2])\n",
    "    kin3 = nltk.word_tokenize(lines[stress_line +7])[-3:]\n",
    "    kin3 = np.array([float(k) for k in kin3])\n",
    "    \n",
    "    kinetic_stress = np.array([kin1, kin2, kin3])\n",
    "    relax_dict['kinetic_stress'] = kinetic_stress\n",
    "    \n",
    "    #Local stress tensor\n",
    "    local1 = nltk.word_tokenize(lines[stress_line +9])[-3:]\n",
    "    local1 = np.array([float(k) for k in local1])\n",
    "    local2 = nltk.word_tokenize(lines[stress_line +10])[-3:]\n",
    "    local2 = np.array([float(k) for k in local2])\n",
    "    local3 = nltk.word_tokenize(lines[stress_line +11])[-3:]\n",
    "    local3 = np.array([float(k) for k in local3])\n",
    "    \n",
    "    local_stress = np.array([local1, local2, local3])\n",
    "    relax_dict['local_stress'] = local_stress\n",
    "    \n",
    "    #Local stress tensor\n",
    "    nonlocal1 = nltk.word_tokenize(lines[stress_line +13])[-3:]\n",
    "    nonlocal1 = np.array([float(k) for k in nonlocal1])\n",
    "    nonlocal2 = nltk.word_tokenize(lines[stress_line +14])[-3:]\n",
    "    nonlocal2 = np.array([float(k) for k in nonlocal2])\n",
    "    nonlocal3 = nltk.word_tokenize(lines[stress_line +15])[-3:]\n",
    "    nonlocal3 = np.array([float(k) for k in nonlocal3])\n",
    "    \n",
    "    nonlocal_stress = np.array([nonlocal1, nonlocal2, nonlocal3])\n",
    "    relax_dict['nonlocal_stress'] = nonlocal_stress\n",
    "    \n",
    "    #Hartree stress tensor\n",
    "    hart1 = nltk.word_tokenize(lines[stress_line +17])[-3:]\n",
    "    hart1 = np.array([float(k) for k in hart1])\n",
    "    hart2 = nltk.word_tokenize(lines[stress_line +18])[-3:]\n",
    "    hart2 = np.array([float(k) for k in hart2])\n",
    "    hart3 = nltk.word_tokenize(lines[stress_line +19])[-3:]\n",
    "    hart3 = np.array([float(k) for k in hart3])\n",
    "    \n",
    "    hartree_stress = np.array([hart1, hart2, hart3])\n",
    "    relax_dict['hartree_stress'] = hartree_stress\n",
    "    \n",
    "    #Exchange correlation stress tensor\n",
    "    excor1 = nltk.word_tokenize(lines[stress_line +21])[-3:]\n",
    "    excor1 = np.array([float(k) for k in excor1])\n",
    "    excor2 = nltk.word_tokenize(lines[stress_line +22])[-3:]\n",
    "    excor2 = np.array([float(k) for k in excor2])\n",
    "    excor3 = nltk.word_tokenize(lines[stress_line +23])[-3:]\n",
    "    excor3 = np.array([float(k) for k in excor3])\n",
    "    \n",
    "    excor_stress = np.array([excor1, excor2, excor3])\n",
    "    relax_dict['excor_stress'] = excor_stress\n",
    "    \n",
    "    #Core correction stress tensor\n",
    "    corecor1 = nltk.word_tokenize(lines[stress_line +25])[-3:]\n",
    "    corecor1 = np.array([float(k) for k in corecor1])\n",
    "    corecor2 = nltk.word_tokenize(lines[stress_line +26])[-3:]\n",
    "    corecor2 = np.array([float(k) for k in corecor2])\n",
    "    corecor3 = nltk.word_tokenize(lines[stress_line +27])[-3:]\n",
    "    corecor3 = np.array([float(k) for k in corecor3])\n",
    "    \n",
    "    corecor_stress = np.array([corecor1, corecor2, corecor3])\n",
    "    relax_dict['corecor_stress'] = corecor_stress\n",
    "    \n",
    "    #Ewald stress tensor\n",
    "    ewald1 = nltk.word_tokenize(lines[stress_line +29])[-3:]\n",
    "    ewald1 = np.array([float(k) for k in ewald1])\n",
    "    ewald2 = nltk.word_tokenize(lines[stress_line +30])[-3:]\n",
    "    ewald2 = np.array([float(k) for k in ewald2])\n",
    "    ewald3 = nltk.word_tokenize(lines[stress_line +31])[-3:]\n",
    "    ewald3 = np.array([float(k) for k in ewald3])\n",
    "    \n",
    "    ewald_stress = np.array([ewald1, ewald2, ewald3])\n",
    "    relax_dict['ewald_stress'] = ewald_stress\n",
    "    \n",
    "    #Hubbard stress tensor\n",
    "    hub1 = nltk.word_tokenize(lines[stress_line +33])[-3:]\n",
    "    hub1 = np.array([float(k) for k in hub1])\n",
    "    hub2 = nltk.word_tokenize(lines[stress_line +34])[-3:]\n",
    "    hub2 = np.array([float(k) for k in hub2])\n",
    "    hub3 = nltk.word_tokenize(lines[stress_line +35])[-3:]\n",
    "    hub3 = np.array([float(k) for k in hub3])\n",
    "    \n",
    "    hubbard_stress = np.array([hub1, hub2, hub3])\n",
    "    relax_dict['hubbard_stress'] = hubbard_stress\n",
    "    \n",
    "    #London stress tensor\n",
    "    lon1 = nltk.word_tokenize(lines[stress_line +37])[-3:]\n",
    "    lon1 = np.array([float(k) for k in lon1])\n",
    "    lon2 = nltk.word_tokenize(lines[stress_line +38])[-3:]\n",
    "    lon2 = np.array([float(k) for k in lon2])\n",
    "    lon3 = nltk.word_tokenize(lines[stress_line +39])[-3:]\n",
    "    lon3 = np.array([float(k) for k in lon3])\n",
    "    \n",
    "    london_stress = np.array([lon1, lon2, lon3])\n",
    "    relax_dict['london_stress'] = london_stress\n",
    "    \n",
    "    #XDM stress tensor\n",
    "    xdm1 = nltk.word_tokenize(lines[stress_line +41])[-3:]\n",
    "    xdm1 = np.array([float(k) for k in xdm1])\n",
    "    xdm2 = nltk.word_tokenize(lines[stress_line +42])[-3:]\n",
    "    xdm2 = np.array([float(k) for k in xdm2])\n",
    "    xdm3 = nltk.word_tokenize(lines[stress_line +43])[-3:]\n",
    "    xdm3 = np.array([float(k) for k in xdm3])\n",
    "    \n",
    "    xdm_stress = np.array([xdm1, xdm2, xdm3])\n",
    "    relax_dict['xdm_stress'] = xdm_stress\n",
    "    \n",
    "    #DFT-NL stress tensor\n",
    "    dft1 = nltk.word_tokenize(lines[stress_line +45])[-3:]\n",
    "    dft1 = np.array([float(k) for k in dft1])\n",
    "    dft2 = nltk.word_tokenize(lines[stress_line +46])[-3:]\n",
    "    dft2 = np.array([float(k) for k in dft2])\n",
    "    dft3 = nltk.word_tokenize(lines[stress_line +47])[-3:]\n",
    "    dft3 = np.array([float(k) for k in dft3])\n",
    "    \n",
    "    dftnl_stress = np.array([dft1, dft2, dft3])\n",
    "    relax_dict['dftnl_stress'] = dftnl_stress\n",
    "    \n",
    "    #TS-vdW stress tensor\n",
    "    vdw1 = nltk.word_tokenize(lines[stress_line +45])[-3:]\n",
    "    vdw1 = np.array([float(k) for k in vdw1])\n",
    "    vdw2 = nltk.word_tokenize(lines[stress_line +46])[-3:]\n",
    "    vdw2 = np.array([float(k) for k in vdw2])\n",
    "    vdw3 = nltk.word_tokenize(lines[stress_line +47])[-3:]\n",
    "    vdw3 = np.array([float(k) for k in vdw3])\n",
    "    \n",
    "    tsvdw_stress = np.array([vdw1, vdw2, vdw3])\n",
    "    relax_dict['tsvdw_stress'] = tsvdw_stress\n",
    "    \n",
    "    force_line = last_line('Forces acting on atoms', relax_output_file)\n",
    "    forces = []\n",
    "    nonlocal_forces = []\n",
    "    ionic_forces = []\n",
    "    local_forces = []\n",
    "    corecor_forces = []\n",
    "    hubbard_forces = []\n",
    "    scf_forces = []\n",
    "    \n",
    "    def get_force_data(line):\n",
    "        data_list = []\n",
    "        line = nltk.word_tokenize(line)\n",
    "        #data_list.append(line[1]) #Atom number\n",
    "        data_list.append(line[3]) #Atomic type\n",
    "        data_list.append(line[-3]) #Force components\n",
    "        data_list.append(line[-2])\n",
    "        data_list.append(line[-1])\n",
    "    \n",
    "        return data_list\n",
    "    \n",
    "    for k in range(natoms):\n",
    "        forces.append(get_force_data(lines[force_line + 2 +k]))\n",
    "        nonlocal_forces.append(get_force_data(lines[force_line + 3+natoms+k]))\n",
    "        ionic_forces.append(get_force_data(lines[force_line + 4+2*natoms+k]))\n",
    "        local_forces.append(get_force_data(lines[force_line + 5+3*natoms+k]))\n",
    "        corecor_forces.append(get_force_data(lines[force_line + 6+4*natoms+k]))\n",
    "        hubbard_forces.append(get_force_data(lines[force_line + 7+5*natoms+k]))\n",
    "        scf_forces.append(get_force_data(lines[force_line + 8+6*natoms+k]))\n",
    "        \n",
    "        \n",
    "    forces = np.array(forces)\n",
    "    relax_dict['forces'] = forces\n",
    "    \n",
    "    nonlocal_forces = np.array(nonlocal_forces)\n",
    "    relax_dict['nonlocal_forces'] = nonlocal_forces\n",
    "    \n",
    "    ionic_forces = np.array(ionic_forces)\n",
    "    relax_dict['ionic_forces'] = ionic_forces\n",
    "    \n",
    "    local_forces = np.array(local_forces)\n",
    "    relax_dict['local_forces'] = local_forces\n",
    "    \n",
    "    corecor_forces = np.array(corecor_forces)\n",
    "    relax_dict['corecor_forces'] = corecor_forces\n",
    "    \n",
    "    hubbard_forces = np.array(hubbard_forces)\n",
    "    relax_dict['hubbard_forces'] = hubbard_forces\n",
    "    \n",
    "    scf_forces = np.array(scf_forces)\n",
    "    relax_dict['scf_forces'] = scf_forces\n",
    "    \n",
    "    return relax_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '0.00000000', '0.00000000', '-0.00000000'],\n",
       "       ['2', '-0.00000001', '-0.00000008', '0.00000007']], dtype='<U11')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relax_dict = get_relax_dict('relax.out', 'picklepickle', 'relaxed_struct')\n",
    "relax_dict['scf_forces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
